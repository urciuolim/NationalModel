{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "840e42230d1047dab29078beff4f9907",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>1</td><td>application_1600108946459_0002</td><td>spark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-23-189.us-east-2.compute.internal:20888/proxy/application_1600108946459_0002/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-18-209.us-east-2.compute.internal:8042/node/containerlogs/container_1600108946459_0002_01_000001/livy\">Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import org.apache.spark.sql.functions.{udf, input_file_name}\n",
      "import org.apache.spark.sql.Row\n",
      "import spark.implicits._\n",
      "extractYear: org.apache.spark.sql.expressions.UserDefinedFunction = UserDefinedFunction(<function1>,StringType,Some(List(StringType)))\n",
      "cbpRaw: org.apache.spark.sql.DataFrame = [State FIPS: string, County FIPS: string ... 15 more fields]\n",
      "+----------+-----------+----+----------+-----+---+---+-----+-----+-----+-------+-------+-------+---------+---------+---------+--------+\n",
      "|State FIPS|County FIPS|Year|NAICS Code|Total|1_4|5_9|10_19|20_49|50_99|100_249|250_499|500_999|1000_1499|1500_2499|2500_4999|5000plus|\n",
      "+----------+-----------+----+----------+-----+---+---+-----+-----+-----+-------+-------+-------+---------+---------+---------+--------+\n",
      "|        01|        001|2011|         0|  835|412|178|  122|   78|   33|     10|      1|      1|        0|        0|        0|       0|\n",
      "|        01|        001|2011|        11|    6|  5|  0|    1|    0|    0|      0|      0|      0|        0|        0|        0|       0|\n",
      "|        01|        001|2011|       113|    5|  4|  0|    1|    0|    0|      0|      0|      0|        0|        0|        0|       0|\n",
      "|        01|        001|2011|      1133|    5|  4|  0|    1|    0|    0|      0|      0|      0|        0|        0|        0|       0|\n",
      "|        01|        001|2011|     11331|    5|  4|  0|    1|    0|    0|      0|      0|      0|        0|        0|        0|       0|\n",
      "|        01|        001|2011|    113310|    5|  4|  0|    1|    0|    0|      0|      0|      0|        0|        0|        0|       0|\n",
      "|        01|        001|2011|       115|    1|  1|  0|    0|    0|    0|      0|      0|      0|        0|        0|        0|       0|\n",
      "|        01|        001|2011|      1151|    1|  1|  0|    0|    0|    0|      0|      0|      0|        0|        0|        0|       0|\n",
      "|        01|        001|2011|     11511|    1|  1|  0|    0|    0|    0|      0|      0|      0|        0|        0|        0|       0|\n",
      "|        01|        001|2011|    115112|    1|  1|  0|    0|    0|    0|      0|      0|      0|        0|        0|        0|       0|\n",
      "|        01|        001|2011|        21|    2|  0|  0|    2|    0|    0|      0|      0|      0|        0|        0|        0|       0|\n",
      "|        01|        001|2011|       212|    2|  0|  0|    2|    0|    0|      0|      0|      0|        0|        0|        0|       0|\n",
      "|        01|        001|2011|      2123|    2|  0|  0|    2|    0|    0|      0|      0|      0|        0|        0|        0|       0|\n",
      "|        01|        001|2011|     21231|    1|  0|  0|    1|    0|    0|      0|      0|      0|        0|        0|        0|       0|\n",
      "|        01|        001|2011|    212319|    1|  0|  0|    1|    0|    0|      0|      0|      0|        0|        0|        0|       0|\n",
      "|        01|        001|2011|     21232|    1|  0|  0|    1|    0|    0|      0|      0|      0|        0|        0|        0|       0|\n",
      "|        01|        001|2011|    212321|    1|  0|  0|    1|    0|    0|      0|      0|      0|        0|        0|        0|       0|\n",
      "|        01|        001|2011|        22|    9|  2|  1|    2|    3|    1|      0|      0|      0|        0|        0|        0|       0|\n",
      "|        01|        001|2011|       221|    9|  2|  1|    2|    3|    1|      0|      0|      0|        0|        0|        0|       0|\n",
      "|        01|        001|2011|      2211|    8|  2|  0|    2|    3|    1|      0|      0|      0|        0|        0|        0|       0|\n",
      "+----------+-----------+----+----------+-----+---+---+-----+-----+-----+-------+-------+-------+---------+---------+---------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "/*\n",
    "* Author: Mike Urciuoli\n",
    "* email: urciuolim@gmail.com\n",
    "* description: The purpose of this notebook is to show how to clean the Census Business Patterns (CBP) dataset for\n",
    "*              usage in a county prediction model. The overall goal of the project is to predict some feature\n",
    "*              of a county based on other input features. Each row of the CBP dataset is a count of how many businesses exist in\n",
    "*              a specific county in a given year (year indicated by the file that row is part of) according to a specifc NAICS\n",
    "*              code. At this point we don't know which NAICS codes the user is interested in, and if a specific county has no\n",
    "*              businesses that fall into a NAICS code category the record is not present in the dataset. So grouping all records\n",
    "*              by county/year will allow us to sense when records are missing for any given county.\n",
    "* \n",
    "*              NAICS codes: https://www.naics.com/naics-code-description/\n",
    "*/\n",
    "\n",
    "import org.apache.spark.sql.functions.{udf, input_file_name}\n",
    "import org.apache.spark.sql.Row\n",
    "import spark.implicits._\n",
    "\n",
    "// Custom UDF that extracts the year from the filename of each Census CBP input file\n",
    "val extractYear = udf((filename: String) => \"20\" + filename.split(\"/\").last.substring(3,5))\n",
    "// Load raw dataset, with added year column, dropping unneeded columns and renaming/casting those remaining\n",
    "val cbpRaw = spark.read.option(\"header\", \"true\").\n",
    "    csv(\"s3://agimodeltrainer/DATA/CBP/\").\n",
    "    withColumn(\"Year\", extractYear(input_file_name())).\n",
    "    drop(List(\"empflag\",\"emp_nf\",\"emp\",\"qp1_nf\",\"qp1\",\"ap_nf\",\"ap\", \"censtate\", \"cencty\", \"n1000\"):_*).\n",
    "    select(\n",
    "        $\"fipstate\".as(\"State FIPS\"),\n",
    "        $\"fipscty\".as(\"County FIPS\"),\n",
    "        $\"Year\".cast(\"int\"),\n",
    "        regexp_replace($\"naics\", \"[-/]\", \"\").as(\"NAICS Code\").cast(\"Int\"),\n",
    "        $\"est\".as(\"Total\").cast(\"int\"),\n",
    "        $\"n1_4\".as(\"1_4\").cast(\"int\"),\n",
    "        $\"n5_9\".as(\"5_9\").cast(\"int\"),\n",
    "        $\"n10_19\".as(\"10_19\").cast(\"int\"),\n",
    "        $\"n20_49\".as(\"20_49\").cast(\"int\"),\n",
    "        $\"n50_99\".as(\"50_99\").cast(\"int\"),\n",
    "        $\"n100_249\".as(\"100_249\").cast(\"int\"),\n",
    "        $\"n250_499\".as(\"250_499\").cast(\"int\"),\n",
    "        $\"n500_999\".as(\"500_999\").cast(\"int\"),\n",
    "        $\"n1000_1\".as(\"1000_1499\").cast(\"int\"),\n",
    "        $\"n1000_2\".as(\"1500_2499\").cast(\"int\"),\n",
    "        $\"n1000_3\".as(\"2500_4999\").cast(\"int\"),\n",
    "        $\"n1000_4\".as(\"5000plus\").cast(\"int\")\n",
    "    ).na.fill(0)\n",
    "cbpRaw.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddb061a01d514f8c80015396672545bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|NAICS Code|\n",
      "+----------+\n",
      "|         0|\n",
      "|        11|\n",
      "|        21|\n",
      "|        22|\n",
      "|        23|\n",
      "|        31|\n",
      "|        42|\n",
      "|        44|\n",
      "|        48|\n",
      "|        51|\n",
      "|        52|\n",
      "|        53|\n",
      "|        54|\n",
      "|        55|\n",
      "|        56|\n",
      "|        61|\n",
      "|        62|\n",
      "|        71|\n",
      "|        72|\n",
      "|        81|\n",
      "|        99|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cbpRaw.select(\"NAICS Code\").filter($\"NAICS Code\" < 100).distinct.orderBy(\"NAICS Code\").show(21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e2083ebf49e4ccb8ca05921de541b49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cols: Array[String] = Array(Total, 1_4, 5_9, 10_19, 20_49, 50_99, 100_249, 250_499, 500_999, 1000_1499, 1500_2499, 2500_4999, 5000plus)\n",
      "+-------+------------------+------------------+------------------+------------------+-----------------+------------------+\n",
      "|summary|             Total|               1_4|               5_9|             10_19|            20_49|             50_99|\n",
      "+-------+------------------+------------------+------------------+------------------+-----------------+------------------+\n",
      "|  count|          13876272|          13876272|          13876272|          13876272|         13876272|          13876272|\n",
      "|   mean|22.843167386744796|12.420304531361161| 4.203605334343403|2.8697289156626504|1.986593517336645|0.6628389815362512|\n",
      "| stddev|374.57349854462854|217.24034394202823|63.772179406914425|45.116066955231354|33.75968424617814|11.924911741797375|\n",
      "|    min|                 1|                 0|                 0|                 0|                0|                 0|\n",
      "|    max|            275316|            167194|             43400|             30127|            21545|              7463|\n",
      "+-------+------------------+------------------+------------------+------------------+-----------------+------------------+\n",
      "\n",
      "+-------+------------------+-------------------+-------------------+--------------------+--------------------+--------------------+-------------------+\n",
      "|summary|           100_249|            250_499|            500_999|           1000_1499|           1500_2499|           2500_4999|           5000plus|\n",
      "+-------+------------------+-------------------+-------------------+--------------------+--------------------+--------------------+-------------------+\n",
      "|  count|          13876272|           13876272|           13876272|            13876272|            13876272|            13876272|           13876272|\n",
      "|   mean|0.3718709895568493|0.09323534447868996|0.03373290751291125|0.009147629853320835|0.006131185667159018|0.003309822695894113| 0.0013976376363911|\n",
      "| stddev| 6.795846868288134| 1.8158591446184842| 0.7163018604325997| 0.23349418670932337| 0.17774852452155115|  0.1193417031598865|0.10078251833294825|\n",
      "|    min|                 0|                  0|                  0|                   0|                   0|                   0|                  0|\n",
      "|    max|              4103|               1058|                369|                 112|                  70|                  49|                 55|\n",
      "+-------+------------------+-------------------+-------------------+--------------------+--------------------+--------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val cols = Array(\"Total\", \"1_4\", \"5_9\", \"10_19\", \"20_49\", \"50_99\", \"100_249\", \"250_499\", \"500_999\", \"1000_1499\", \"1500_2499\", \"2500_4999\", \"5000plus\")\n",
    "// Description of each of the above columns. A few points:\n",
    "// 1. There is at least one business in each county, as indicated by the min value in the Total column.\n",
    "// 2. The average number of businesses for each category of number of employees (1-4 => 1-4 Employees)\n",
    "//    shrinks as number of employees increases.\n",
    "// These numbers are based on all NAICS codes, and we could get more specific descriptions for a specifc code if we wanted to.\n",
    "\n",
    "// Description show in two halves for formatting reasons\n",
    "cbpRaw.describe(cols.slice(0, cols.size/2):_*).show\n",
    "cbpRaw.describe(cols.slice(cols.size/2, cols.size):_*).show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2b93cccf171480a9850e856e8281a77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "condenseRow: (r: org.apache.spark.sql.Row)(String, Int, scala.collection.immutable.Map[Int,Seq[Int]])\n",
      "cbpMaps: org.apache.spark.sql.DataFrame = [FIPS: string, Year: int ... 1 more field]\n",
      "root\n",
      " |-- FIPS: string (nullable = true)\n",
      " |-- Year: integer (nullable = false)\n",
      " |-- Record: map (nullable = true)\n",
      " |    |-- key: integer\n",
      " |    |-- value: array (valueContainsNull = true)\n",
      " |    |    |-- element: integer (containsNull = false)\n",
      "\n",
      "+-----+----+---------------------------------------------------------+\n",
      "|FIPS |Year|Record                                                   |\n",
      "+-----+----+---------------------------------------------------------+\n",
      "|01001|2011|[0 -> [835, 412, 178, 122, 78, 33, 10, 1, 1, 0, 0, 0, 0]]|\n",
      "|01001|2011|[11 -> [6, 5, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]]          |\n",
      "|01001|2011|[113 -> [5, 4, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]]         |\n",
      "|01001|2011|[1133 -> [5, 4, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]]        |\n",
      "|01001|2011|[11331 -> [5, 4, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]]       |\n",
      "|01001|2011|[113310 -> [5, 4, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]]      |\n",
      "|01001|2011|[115 -> [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]         |\n",
      "|01001|2011|[1151 -> [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]        |\n",
      "|01001|2011|[11511 -> [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]       |\n",
      "|01001|2011|[115112 -> [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]      |\n",
      "|01001|2011|[21 -> [2, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0]]          |\n",
      "|01001|2011|[212 -> [2, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0]]         |\n",
      "|01001|2011|[2123 -> [2, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0]]        |\n",
      "|01001|2011|[21231 -> [1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]]       |\n",
      "|01001|2011|[212319 -> [1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]]      |\n",
      "|01001|2011|[21232 -> [1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]]       |\n",
      "|01001|2011|[212321 -> [1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]]      |\n",
      "|01001|2011|[22 -> [9, 2, 1, 2, 3, 1, 0, 0, 0, 0, 0, 0, 0]]          |\n",
      "|01001|2011|[221 -> [9, 2, 1, 2, 3, 1, 0, 0, 0, 0, 0, 0, 0]]         |\n",
      "|01001|2011|[2211 -> [8, 2, 0, 2, 3, 1, 0, 0, 0, 0, 0, 0, 0]]        |\n",
      "+-----+----+---------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// Condense the above columns into one map record, where the key is the NAICS code of that row\n",
    "// and the value is an array of int counts, where each count pertains to the number of businesses\n",
    "// for that county/year/NAICS code of a specific employee size\n",
    "def condenseRow(r:Row) = {\n",
    "    val statefips = r.getString(r.fieldIndex(\"State FIPS\"))\n",
    "    val countyfips = r.getString(r.fieldIndex(\"County FIPS\"))\n",
    "    val year = r.getInt(r.fieldIndex(\"Year\"))\n",
    "    val naics = r.getInt(r.fieldIndex(\"NAICS Code\"))\n",
    "    val startIndex = r.fieldIndex(\"Total\")\n",
    "    val endIndex = r.fieldIndex(\"5000plus\")\n",
    "    val record = r.toSeq.slice(startIndex, endIndex+1).map(_.toString.toInt)\n",
    "    (\n",
    "        statefips+countyfips,\n",
    "        year, \n",
    "        Map(naics->record)\n",
    "    )\n",
    "}\n",
    "// Apply the above function to each raw row, see below for example output\n",
    "val cbpMaps = cbpRaw.map(condenseRow(_)).select(\n",
    "    $\"_1\".as(\"FIPS\"),\n",
    "    $\"_2\".as(\"Year\"),\n",
    "    $\"_3\".as(\"Record\")\n",
    ")\n",
    "cbpMaps.printSchema\n",
    "cbpMaps.show(false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "738f7c6ff6fa420b8f93b3d8b92382cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seqToMap: org.apache.spark.sql.expressions.UserDefinedFunction = UserDefinedFunction(<function1>,MapType(IntegerType,ArrayType(IntegerType,false),true),Some(List(ArrayType(MapType(IntegerType,ArrayType(IntegerType,false),true),true))))\n",
      "cbpMapsAgg: org.apache.spark.sql.DataFrame = [FIPS: string, Year: int ... 1 more field]\n",
      "root\n",
      " |-- FIPS: string (nullable = true)\n",
      " |-- Year: integer (nullable = false)\n",
      " |-- Maps: map (nullable = true)\n",
      " |    |-- key: integer\n",
      " |    |-- value: array (valueContainsNull = true)\n",
      " |    |    |-- element: integer (containsNull = false)\n",
      "\n",
      "+-----+----+--------------------+\n",
      "| FIPS|Year|                Maps|\n",
      "+-----+----+--------------------+\n",
      "|01003|2016|[532490 -> [5, 1,...|\n",
      "|01023|2013|[484110 -> [3, 3,...|\n",
      "|01025|2017|[62161 -> [7, 0, ...|\n",
      "|01027|2016|[484110 -> [2, 1,...|\n",
      "|01037|2017|[0 -> [96, 71, 9,...|\n",
      "|01049|2011|[484110 -> [2, 1,...|\n",
      "|01049|2014|[484110 -> [1, 0,...|\n",
      "|01055|2011|[532490 -> [1, 1,...|\n",
      "|01059|2015|[484110 -> [2, 1,...|\n",
      "|01067|2016|[484110 -> [2, 1,...|\n",
      "|01073|2016|[532490 -> [32, 1...|\n",
      "|01079|2012|[484110 -> [4, 3,...|\n",
      "|01079|2017|[62161 -> [3, 0, ...|\n",
      "|01081|2015|[532490 -> [4, 4,...|\n",
      "|01089|2014|[532490 -> [4, 3,...|\n",
      "|01097|2012|[532490 -> [19, 7...|\n",
      "|01101|2014|[532490 -> [5, 4,...|\n",
      "|01113|2013|[62161 -> [3, 1, ...|\n",
      "|01117|2013|[532490 -> [7, 1,...|\n",
      "|01125|2011|[532490 -> [1, 0,...|\n",
      "+-----+----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// I couldn't find a built-in way to join maps in a spark reduce-like function, so a way to do this\n",
    "// would be to group rows by FIPS and Year, then aggregate each individual map into a list via\n",
    "// the collect_list aggregate function. We can then use a custom UDF to turn these lists into\n",
    "// maps as intended. A more straightfoward way to do this is to define a custom UDAF, but that\n",
    "// would require writing more code than this one.\n",
    "val seqToMap = udf((seq: Seq[Map[Int, Seq[Int]]]) => seq.flatten.toMap)\n",
    "val cbpMapsAgg = cbpMaps.groupBy(\"FIPS\", \"Year\").\n",
    "    agg(collect_list(\"Record\").alias(\"Records\")).\n",
    "    withColumn(\"Maps\", seqToMap($\"Records\")).\n",
    "    drop(\"Records\")\n",
    "cbpMapsAgg.printSchema\n",
    "cbpMapsAgg.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbd9926ea5374e16b5b8ee1d7a6fd28c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res32: Long = 22338\n",
      "res33: Long = 3198\n",
      "res34: Long = 7\n"
     ]
    }
   ],
   "source": [
    "// Quick check to make sure the number of rows is expected. There are about 3,141 counties in the US, plus some\n",
    "// U.S. territory equivalents\n",
    "cbpMapsAgg.count\n",
    "cbpMapsAgg.select(\"FIPS\").distinct.count\n",
    "cbpMapsAgg.select(\"Year\").distinct.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22c5aa80911346e0b3475d453c6f1bd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "org.apache.spark.sql.AnalysisException: path s3://agimodeltrainer/Clean_Data/Census_Business_Patterns.parquet already exists.;\n",
      "  at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:119)\n",
      "  at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:104)\n",
      "  at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:102)\n",
      "  at org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:122)\n",
      "  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:173)\n",
      "  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:169)\n",
      "  at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:197)\n",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:194)\n",
      "  at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:169)\n",
      "  at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:114)\n",
      "  at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:112)\n",
      "  at org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)\n",
      "  at org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)\n",
      "  at org.apache.spark.sql.execution.SQLExecution$.org$apache$spark$sql$execution$SQLExecution$$executeQuery$1(SQLExecution.scala:83)\n",
      "  at org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1$$anonfun$apply$1.apply(SQLExecution.scala:94)\n",
      "  at org.apache.spark.sql.execution.QueryExecutionMetrics$.withMetrics(QueryExecutionMetrics.scala:141)\n",
      "  at org.apache.spark.sql.execution.SQLExecution$.org$apache$spark$sql$execution$SQLExecution$$withMetrics(SQLExecution.scala:178)\n",
      "  at org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:93)\n",
      "  at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:200)\n",
      "  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:92)\n",
      "  at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:676)\n",
      "  at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:285)\n",
      "  at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:271)\n",
      "  at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:229)\n",
      "  at org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:566)\n",
      "  ... 52 elided\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// Store our final table in S3 for usage later in the application.\n",
    "cbpMapsAgg.write.parquet(\"s3://agimodeltrainer/Clean_Data/Census_Business_Patterns.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff122705c7f54f17b6125a3b2f9f19de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test: org.apache.spark.sql.DataFrame = [FIPS: string, Year: int ... 1 more field]\n",
      "root\n",
      " |-- FIPS: string (nullable = true)\n",
      " |-- Year: integer (nullable = true)\n",
      " |-- Maps: map (nullable = true)\n",
      " |    |-- key: integer\n",
      " |    |-- value: array (valueContainsNull = true)\n",
      " |    |    |-- element: integer (containsNull = true)\n",
      "\n",
      "+-----+----+--------------------+\n",
      "| FIPS|Year|                Maps|\n",
      "+-----+----+--------------------+\n",
      "|01001|2016|[532490 -> [1, 1,...|\n",
      "|01005|2012|[484110 -> [1, 0,...|\n",
      "|01007|2016|[484110 -> [5, 2,...|\n",
      "|01019|2015|[484110 -> [1, 0,...|\n",
      "|01025|2014|[484110 -> [5, 2,...|\n",
      "|01033|2017|[484110 -> [3, 0,...|\n",
      "|01071|2011|[62161 -> [4, 1, ...|\n",
      "|01075|2014|[532490 -> [1, 1,...|\n",
      "|01083|2013|[484110 -> [5, 4,...|\n",
      "|01085|2014|[484110 -> [1, 1,...|\n",
      "|01101|2017|[532490 -> [3, 0,...|\n",
      "|01105|2015|[532490 -> [1, 1,...|\n",
      "|01105|2017|[0 -> [121, 70, 2...|\n",
      "|01129|2011|[484110 -> [2, 2,...|\n",
      "|02070|2017|[0 -> [98, 59, 20...|\n",
      "|02195|2016|[484110 -> [1, 1,...|\n",
      "|02240|2017|[0 -> [177, 103, ...|\n",
      "|04005|2014|[532490 -> [3, 2,...|\n",
      "|04007|2012|[532490 -> [2, 1,...|\n",
      "|04017|2013|[532490 -> [1, 1,...|\n",
      "+-----+----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "res40: Long = 22338\n",
      "res41: Long = 3198\n",
      "res42: Long = 7\n"
     ]
    }
   ],
   "source": [
    "// Quick test to show that data was written successfully. Notice that schema is maintained after write/read due to parquet file.\n",
    "val test = spark.read.parquet(\"s3://agimodeltrainer/Clean_Data/Census_Business_Patterns.parquet\")\n",
    "test.printSchema\n",
    "test.show\n",
    "test.count\n",
    "test.select(\"FIPS\").distinct.count\n",
    "test.select(\"Year\").distinct.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63090df70aff4e36a3b31e513a5ed8ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step1: Array[org.apache.spark.sql.Row] = Array([01001,2016,Map(532490 -> WrappedArray(1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0), 484110 -> WrappedArray(1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0), 62161 -> WrappedArray(2, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0), 53139 -> WrappedArray(1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0), 6231 -> WrappedArray(2, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0), 3399 -> WrappedArray(2, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0), 8114 -> WrappedArray(5, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0), 333 -> WrappedArray(1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0), 722514 -> WrappedArray(3, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0), 518 -> WrappedArray(1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0), 448140 -> WrappedArray(6, 2, 0, 1, 3, 0, 0, 0, 0, 0, 0, 0, 0), 52213 -> WrappedArray(5, 1, 3, 1, 0, 0, 0, 0, 0, 0, 0, ..."
     ]
    }
   ],
   "source": [
    "val step1 = test.filter($\"Year\" === 2016).filter($\"FIPS\" === \"01001\").collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26882a4853b5429f9e0eb8ce6fa45622",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res43: Seq[Int] = WrappedArray(851, 404, 186, 126, 97, 27, 8, 2, 1, 0, 0, 0, 0)\n",
      "-----------------------------------\n",
      "res45: Seq[Int] = WrappedArray(9, 6, 2, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0)\n"
     ]
    }
   ],
   "source": [
    "step1(0).getAs[Map[Int, Seq[Int]]](2)(0)\n",
    "println(\"-----------------------------------\")\n",
    "step1(0).getAs[Map[Int, Seq[Int]]](2)(11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark",
   "language": "",
   "name": "sparkkernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
